{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e61dff-4588-44cc-9041-e0964d64ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Où Windows trouve hadoop.dll ?\n",
      "Information : impossible de trouver des fichiers pour le(s) modèle(s) spécifié(s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, textwrap\n",
    "\n",
    "print(\"Où Windows trouve hadoop.dll ?\")\n",
    "res = subprocess.run([\"where\", \"hadoop.dll\"], capture_output=True, text=True)\n",
    "print(res.stdout if res.stdout else res.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439708a-f6cf-42bb-9c3e-86c6ef9838b9",
   "metadata": {},
   "source": [
    "# Définir HADOOP_HOME/PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107ebcd8-b9e5-4c74-8e77-2c39eaa96642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME = C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\n",
      "BIN = C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HADOOP_HOME = (Path.cwd() / \"hadoop\").resolve()\n",
    "BIN = HADOOP_HOME / \"bin\"\n",
    "\n",
    "WINUTILS = BIN / \"winutils.exe\"\n",
    "HADOOP_DLL = BIN / \"hadoop.dll\"\n",
    "\n",
    "assert WINUTILS.exists(), f\"winutils.exe introuvable: {WINUTILS}\"\n",
    "assert HADOOP_DLL.exists(), f\"hadoop.dll introuvable: {HADOOP_DLL}\"\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = str(HADOOP_HOME)\n",
    "os.environ[\"hadoop.home.dir\"] = str(HADOOP_HOME)\n",
    "os.environ[\"PATH\"] = str(BIN) + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "print(\"HADOOP_HOME =\", os.environ[\"HADOOP_HOME\"])\n",
    "print(\"BIN =\", BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71292980-3e7b-48f3-a090-3971b27ecdef",
   "metadata": {},
   "source": [
    "# Paramètres + vérification des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ace11e-2460-480e-ad22-a342d26a8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\n",
      "Fichiers présents: ['.ipynb_checkpoints', 'apprentissage_pyspark.ipynb', 'cities.csv', 'dataset_final_immo_idf_parquet', 'hadoop', 'idf_ventes.csv', 'nettoyage_pyspark.ipynb', 'scrap.py']\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 — Paramètres + vérification des fichiers\n",
    "import os\n",
    "\n",
    "RAW_ADS_PATH = \"idf_ventes.csv\"\n",
    "CITIES_PATH  = \"cities.csv\"\n",
    "\n",
    "# Dataset final sauvegardé (format Parquet recommandé)\n",
    "OUTPUT_DATASET_PATH = \"dataset_final_immo_idf_parquet\"\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Fichiers présents:\", os.listdir(\".\"))\n",
    "\n",
    "assert os.path.exists(RAW_ADS_PATH), f\"Fichier introuvable: {RAW_ADS_PATH}\"\n",
    "assert os.path.exists(CITIES_PATH),  f\"Fichier introuvable: {CITIES_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cee446-8b65-4a83-b280-d25fa0934966",
   "metadata": {},
   "source": [
    "# Démarrage Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28304bfc-a798-413b-89d6-b5e4aa6f7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 2 — Démarrage Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"ImmoIDF-Nettoyage\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a867ae-f42e-43ae-8688-109d647d7750",
   "metadata": {},
   "source": [
    "# Lecture du CSV des annonces + trim + contrôles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f67d038-0911-4b9e-bb56-47868e5cab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes: ['Ville', 'Type', 'Surface', 'NbrPieces', 'NbrChambres', 'NbrSdb', 'DPE', 'Prix']\n",
      "Schéma (brut):\n",
      "root\n",
      " |-- Ville: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Surface: string (nullable = true)\n",
      " |-- NbrPieces: string (nullable = true)\n",
      " |-- NbrChambres: string (nullable = true)\n",
      " |-- NbrSdb: string (nullable = true)\n",
      " |-- DPE: string (nullable = true)\n",
      " |-- Prix: string (nullable = true)\n",
      "\n",
      "Aperçu:\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "|Ville             |Type  |Surface|NbrPieces|NbrChambres|NbrSdb|DPE   |Prix  |\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "|Paris 15ème       |Maison|220    |8        |4          |3     |Vierge|250000|\n",
      "|Meaux             |Maison|325    |10       |5          |-     |-     |749000|\n",
      "|Misy-sur-Yonne    |Maison|128    |5        |4          |1     |E     |279000|\n",
      "|Nanteuil-lès-Meaux|Maison|89     |5        |3          |-     |F     |242000|\n",
      "|Villeparisis      |Maison|65     |3        |2          |1     |B     |289000|\n",
      "|Bondy             |Maison|118    |5        |3          |1     |B     |500000|\n",
      "|Montreuil         |Maison|160    |7        |4          |2     |E     |695000|\n",
      "|Argenteuil        |Maison|43     |2        |1          |-     |C     |199000|\n",
      "|Eaubonne          |Maison|94     |5        |3          |1     |G     |410000|\n",
      "|Franconville      |Maison|190    |7        |4          |2     |C     |599990|\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Nb lignes: 530\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3 — Lecture du CSV annonces + trim + contrôles\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "EXPECTED_COLS = [\"Ville\", \"Type\", \"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"DPE\", \"Prix\"]\n",
    "\n",
    "annonces_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"utf-8\")\n",
    "    .csv(RAW_ADS_PATH)\n",
    ")\n",
    "\n",
    "# Trim sur toutes les colonnes\n",
    "annonces = annonces_raw\n",
    "for c in annonces.columns:\n",
    "    annonces = annonces.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "print(\"Colonnes:\", annonces.columns)\n",
    "missing = [c for c in EXPECTED_COLS if c not in annonces.columns]\n",
    "assert len(missing) == 0, f\"Colonnes manquantes dans {RAW_ADS_PATH}: {missing}\"\n",
    "\n",
    "print(\"Schéma (brut):\")\n",
    "annonces.printSchema()\n",
    "\n",
    "print(\"Aperçu:\")\n",
    "annonces.show(10, truncate=False)\n",
    "\n",
    "print(\"Nb lignes:\", annonces.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43282e50-1329-4e90-9e26-814218a18c16",
   "metadata": {},
   "source": [
    "# Nettoyage des catégories (Type, DPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903ac9ba-9bba-4ae6-97ff-ca88ea21e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types distincts:\n",
      "+-----------+\n",
      "|Type       |\n",
      "+-----------+\n",
      "|Appartement|\n",
      "|Maison     |\n",
      "+-----------+\n",
      "\n",
      "DPE distincts:\n",
      "+------+\n",
      "|DPE   |\n",
      "+------+\n",
      "|F     |\n",
      "|E     |\n",
      "|B     |\n",
      "|D     |\n",
      "|C     |\n",
      "|Vierge|\n",
      "|G     |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 4 — Nettoyage catégories (Type, DPE)\n",
    "# Objectif :\n",
    "# - Type ∈ {\"Maison\",\"Appartement\"} sinon null\n",
    "# - DPE ∈ {\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"} sinon \"Vierge\"\n",
    "annonces = (annonces\n",
    "    .withColumn(\"Type\", F.when(F.col(\"Type\").isin(\"Maison\", \"Appartement\"), F.col(\"Type\")).otherwise(F.lit(None)))\n",
    "    .withColumn(\"DPE\", F.upper(F.col(\"DPE\")))\n",
    "    .withColumn(\"DPE\", F.when(F.col(\"DPE\").isin(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"), F.col(\"DPE\")).otherwise(F.lit(\"Vierge\")))\n",
    ")\n",
    "\n",
    "# On enlève les lignes sans Type valide (sinon vos dummies Type seront incohérentes)\n",
    "annonces = annonces.dropna(subset=[\"Type\"])\n",
    "\n",
    "print(\"Types distincts:\")\n",
    "annonces.select(\"Type\").distinct().show(truncate=False)\n",
    "\n",
    "print(\"DPE distincts:\")\n",
    "annonces.select(\"DPE\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfadf71-e926-4487-8514-ab59c8f52ccf",
   "metadata": {},
   "source": [
    "# Nettoyage des numériques (Surface, pièces, chambres, sdb, prix) + cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a33dd8e-1f26-42b2-800e-b4288a7eb066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schéma après cast:\n",
      "root\n",
      " |-- Ville: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Surface: double (nullable = true)\n",
      " |-- NbrPieces: double (nullable = true)\n",
      " |-- NbrChambres: double (nullable = true)\n",
      " |-- NbrSdb: double (nullable = true)\n",
      " |-- DPE: string (nullable = true)\n",
      " |-- Prix: double (nullable = true)\n",
      "\n",
      "Null counts (numériques):\n",
      "+------------+--------------+----------------+-----------+---------+\n",
      "|null_Surface|null_NbrPieces|null_NbrChambres|null_NbrSdb|null_Prix|\n",
      "+------------+--------------+----------------+-----------+---------+\n",
      "|22          |21            |47              |86         |0        |\n",
      "+------------+--------------+----------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 5 — Nettoyage numériques + cast\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "num_cols = [\"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"Prix\"]\n",
    "\n",
    "for c in num_cols:\n",
    "    annonces = (annonces\n",
    "        .withColumn(\n",
    "            c,\n",
    "            F.when((F.col(c) == \"-\") | (F.col(c) == \"\") | F.col(c).isNull(), F.lit(None))\n",
    "             .otherwise(F.col(c))\n",
    "        )\n",
    "        .withColumn(c, F.col(c).cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "print(\"Schéma après cast:\")\n",
    "annonces.printSchema()\n",
    "\n",
    "print(\"Null counts (numériques):\")\n",
    "annonces.select([F.count(F.when(F.col(c).isNull(), 1)).alias(f\"null_{c}\") for c in num_cols]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779b55a-c1d9-40b3-a905-de0f9498d932",
   "metadata": {},
   "source": [
    "# Imputation par moyenne (numériques) + dropna résiduel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89949e77-6e9f-4578-9a4a-cd5c710ede3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyennes calculées: {'Surface': 91.27362204724409, 'NbrPieces': 5.5265225933202355, 'NbrChambres': 2.525879917184265, 'NbrSdb': 1.2905405405405406, 'Prix': 1.4153816070518901e+29}\n",
      "Nb lignes après imputation/dropna: 530\n"
     ]
    }
   ],
   "source": [
    "# Cellule 6 — Imputation par moyenne + dropna résiduel\n",
    "means_row = annonces.agg(*[F.avg(F.col(c)).alias(c) for c in num_cols]).collect()[0]\n",
    "means = {c: means_row[c] for c in num_cols}\n",
    "print(\"Moyennes calculées:\", means)\n",
    "\n",
    "for c in num_cols:\n",
    "    # Si la moyenne est None (colonne vide), on ne peut pas imputer -> on laissera en null\n",
    "    if means[c] is not None:\n",
    "        annonces = annonces.withColumn(c, F.when(F.col(c).isNull(), F.lit(means[c])).otherwise(F.col(c)))\n",
    "\n",
    "# Si des nulls restent (ex: moyenne None), on supprime les lignes concernées\n",
    "annonces = annonces.dropna(subset=num_cols)\n",
    "\n",
    "print(\"Nb lignes après imputation/dropna:\", annonces.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb1f9f-453b-4929-93dd-204668687e3c",
   "metadata": {},
   "source": [
    "# Création des dummies Type et DPE (colonnes fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c64e31-1b6c-43a2-8c12-9a6f13a462cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+----------------+-----+-----+-----+-----+-----+-----+----------+\n",
      "|Type  |DPE   |Type_Maison|Type_Appartement|DPE_B|DPE_C|DPE_D|DPE_E|DPE_F|DPE_G|DPE_Vierge|\n",
      "+------+------+-----------+----------------+-----+-----+-----+-----+-----+-----+----------+\n",
      "|Maison|Vierge|1          |0               |0    |0    |0    |0    |0    |0    |1         |\n",
      "|Maison|Vierge|1          |0               |0    |0    |0    |0    |0    |0    |1         |\n",
      "|Maison|E     |1          |0               |0    |0    |0    |1    |0    |0    |0         |\n",
      "|Maison|F     |1          |0               |0    |0    |0    |0    |1    |0    |0         |\n",
      "|Maison|B     |1          |0               |1    |0    |0    |0    |0    |0    |0         |\n",
      "|Maison|B     |1          |0               |1    |0    |0    |0    |0    |0    |0         |\n",
      "|Maison|E     |1          |0               |0    |0    |0    |1    |0    |0    |0         |\n",
      "|Maison|C     |1          |0               |0    |1    |0    |0    |0    |0    |0         |\n",
      "|Maison|G     |1          |0               |0    |0    |0    |0    |0    |1    |0         |\n",
      "|Maison|C     |1          |0               |0    |1    |0    |0    |0    |0    |0         |\n",
      "+------+------+-----------+----------------+-----+-----+-----+-----+-----+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 7 — Création des dummies (colonnes fixes, stables)\n",
    "# Type\n",
    "annonces = (annonces\n",
    "    .withColumn(\"Type_Maison\",       F.when(F.col(\"Type\") == \"Maison\", 1).otherwise(0).cast(\"int\"))\n",
    "    .withColumn(\"Type_Appartement\", F.when(F.col(\"Type\") == \"Appartement\", 1).otherwise(0).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# DPE : on garde B..G + Vierge (comme dans votre sélection finale)\n",
    "for v in [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"Vierge\"]:\n",
    "    annonces = annonces.withColumn(f\"DPE_{v}\", F.when(F.col(\"DPE\") == v, 1).otherwise(0).cast(\"int\"))\n",
    "\n",
    "# Contrôle rapide\n",
    "cols_to_show = [\"Type\",\"DPE\",\"Type_Maison\",\"Type_Appartement\"] + [f\"DPE_{v}\" for v in [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"Vierge\"]]\n",
    "annonces.select(cols_to_show).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9b44d-1289-4efc-8a5a-71cdcfbb8f7d",
   "metadata": {},
   "source": [
    "# Lecture cities.csv (détection séparateur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae367e85-e17f-4192-a0b4-0b9af41029fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes villes: ['insee_code', 'city_code', 'zip_code', 'label', 'latitude', 'longitude', 'department_name', 'department_number', 'region_name', 'region_geojson_name']\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "|insee_code|city_code          |zip_code|label              |latitude    |longitude  |department_name|department_number|region_name            |region_geojson_name    |\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "|25620     |ville du pont      |25650   |ville du pont      |46.999873398|6.498147193|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25624     |villers grelot     |25640   |villers grelot     |47.361512085|6.235167025|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25615     |villars les blamont|25310   |villars les blamont|47.368383721|6.871414913|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25619     |les villedieu      |25240   |les villedieu      |46.713906258|6.26583065 |doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25622     |villers buzon      |25170   |villers buzon      |47.228558434|5.852186748|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 8 — Lecture cities.csv (détection séparateur)\n",
    "villes = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"encoding\", \"utf-8\")\n",
    "          .option(\"sep\", \";\")\n",
    "          .csv(CITIES_PATH))\n",
    "\n",
    "# Si une seule colonne, on retente avec ','\n",
    "if len(villes.columns) == 1:\n",
    "    villes = (spark.read\n",
    "              .option(\"header\", True)\n",
    "              .option(\"encoding\", \"utf-8\")\n",
    "              .option(\"sep\", \",\")\n",
    "              .csv(CITIES_PATH))\n",
    "\n",
    "print(\"Colonnes villes:\", villes.columns)\n",
    "villes.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b1067-bd8f-410d-bfe1-ca7746c3fdb0",
   "metadata": {},
   "source": [
    "# UDF de normalisation des noms de ville (clé de jointure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811b02b1-e711-48b2-9523-7a11e052aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 9 — Normalisation des villes (clé de jointure)\n",
    "import unicodedata\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def strip_accents_py(s: str) -> str:\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "strip_accents = F.udf(strip_accents_py, StringType())\n",
    "\n",
    "def add_city_key(df, colname: str, out: str = \"Ville_key\"):\n",
    "    x = F.lower(F.trim(F.col(colname)))\n",
    "    x = strip_accents(x)\n",
    "    x = F.regexp_replace(x, r\"\\b\\d+\\s*(er|e|eme|ème)\\b\", \"\")   # arrondissements\n",
    "    x = F.regexp_replace(x, r\"[^0-9a-z]+\", \" \")               # séparateurs -> espace\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    x = F.trim(x)\n",
    "    return df.withColumn(out, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefc895-06b2-460d-9b57-e109992c447a",
   "metadata": {},
   "source": [
    "# Filtre IDF + mapping Ville_key -> (lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a300b6-e746-41cb-b28e-e0a3bb523051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+\n",
      "|Ville_key           |latitude    |longitude  |\n",
      "+--------------------+------------+-----------+\n",
      "|barbizon            |48.448347603|2.600809608|\n",
      "|cergy               |49.039967587|2.051139021|\n",
      "|coulommiers         |48.81234041 |3.091269785|\n",
      "|reau                |48.60739655 |2.623860846|\n",
      "|fontenay le vicomte |48.547225879|2.400248804|\n",
      "|janvry              |48.646397715|2.15459332 |\n",
      "|marolles en hurepoix|48.564568271|2.299591744|\n",
      "|berville            |49.192304503|2.070134653|\n",
      "|le plessis gassot   |49.035986122|2.414849753|\n",
      "|lognes              |48.833863469|2.634038048|\n",
      "+--------------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 10 — Filtre IDF + mapping Ville_key -> (lat, lon)\n",
    "IDF_DEPTS = [\"75\",\"77\",\"78\",\"91\",\"92\",\"93\",\"94\",\"95\"]\n",
    "\n",
    "villes_idf = villes.filter(F.col(\"department_number\").isin(IDF_DEPTS))\n",
    "\n",
    "villes_k = (add_city_key(villes_idf, \"label\", \"Ville_key\")\n",
    "    .select(\n",
    "        \"Ville_key\",\n",
    "        F.col(\"latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "        F.col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "    )\n",
    "    # Si plusieurs lignes pour une même ville, on moyenne\n",
    "    .groupBy(\"Ville_key\")\n",
    "    .agg(F.avg(\"latitude\").alias(\"latitude\"), F.avg(\"longitude\").alias(\"longitude\"))\n",
    ")\n",
    "\n",
    "villes_k.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6c69e-d3e7-4bc8-9672-179202f48b6f",
   "metadata": {},
   "source": [
    "# Jointure annonces ↔ villes + corrections de cas particuliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac81bde-555c-4c44-90a2-0d90d0d79ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes annonces_geo: 530\n",
      "Nb lignes sans coordonnées: 27\n",
      "Nb lignes après suppression lat/lon manquants: 503\n"
     ]
    }
   ],
   "source": [
    "# Cellule 11 — Jointure annonces ↔ villes + corrections de cas particuliers\n",
    "annonces_tmp = add_city_key(annonces, \"Ville\", \"Ville_key\")\n",
    "\n",
    "# Cas particuliers (comme dans votre notebook)\n",
    "annonces_tmp = annonces_tmp.withColumn(\n",
    "    \"Ville_key\",\n",
    "    F.when(F.col(\"Ville_key\").isin(\"evry\", \"courcouronnes\"), F.lit(\"evry courcouronnes\"))\n",
    "     .when(F.col(\"Ville_key\") == \"le chesnay\", F.lit(\"le chesnay rocquencourt\"))\n",
    "     .otherwise(F.col(\"Ville_key\"))\n",
    ")\n",
    "\n",
    "annonces_geo = annonces_tmp.join(villes_k, on=\"Ville_key\", how=\"left\")\n",
    "\n",
    "print(\"Nb lignes annonces_geo:\", annonces_geo.count())\n",
    "print(\"Nb lignes sans coordonnées:\",\n",
    "      annonces_geo.filter(F.col(\"latitude\").isNull() | F.col(\"longitude\").isNull()).count())\n",
    "\n",
    "# On enlève celles sans coordonnées\n",
    "annonces_geo = annonces_geo.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "print(\"Nb lignes après suppression lat/lon manquants:\", annonces_geo.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c53f50-c95f-4351-a45b-ecad1db73575",
   "metadata": {},
   "source": [
    "# Sélection des colonnes finales (dataset ML) + cast + dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a84622a-a6cd-4d41-a44c-35b7d883763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes (final): 503\n",
      "Nb colonnes (final): 16\n",
      "root\n",
      " |-- Surface: double (nullable = true)\n",
      " |-- NbrPieces: double (nullable = true)\n",
      " |-- NbrChambres: double (nullable = true)\n",
      " |-- NbrSdb: double (nullable = true)\n",
      " |-- Prix: double (nullable = true)\n",
      " |-- DPE_B: integer (nullable = false)\n",
      " |-- DPE_C: integer (nullable = false)\n",
      " |-- DPE_D: integer (nullable = false)\n",
      " |-- DPE_E: integer (nullable = false)\n",
      " |-- DPE_F: integer (nullable = false)\n",
      " |-- DPE_G: integer (nullable = false)\n",
      " |-- DPE_Vierge: integer (nullable = false)\n",
      " |-- Type_Appartement: integer (nullable = false)\n",
      " |-- Type_Maison: integer (nullable = false)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+-------+---------+-----------+------------------+---------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|Surface|NbrPieces|NbrChambres|NbrSdb            |Prix     |DPE_B|DPE_C|DPE_D|DPE_E|DPE_F|DPE_G|DPE_Vierge|Type_Appartement|Type_Maison|latitude    |longitude  |\n",
      "+-------+---------+-----------+------------------+---------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|60.0   |3.0      |2.0        |1.0               |215000.0 |0    |0    |0    |0    |0    |0    |1         |1               |0          |48.60739655 |2.623860846|\n",
      "|70.0   |4.0      |2.0        |1.0               |180000.0 |0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|75.0   |2.0      |3.0        |1.0               |115000.0 |0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|70.0   |4.0      |2.0        |1.0               |120000.0 |0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|90.0   |4.0      |3.0        |1.0               |700000.0 |0    |0    |0    |1    |0    |0    |0         |1               |0          |48.836400822|2.239066161|\n",
      "|70.0   |3.0      |2.0        |1.0               |560000.0 |0    |0    |1    |0    |0    |0    |0         |1               |0          |48.836400822|2.239066161|\n",
      "|95.0   |4.0      |3.0        |1.0               |1060000.0|0    |0    |1    |0    |0    |0    |0         |1               |0          |48.836400822|2.239066161|\n",
      "|100.0  |5.0      |4.0        |1.2905405405405406|254400.0 |0    |0    |0    |0    |0    |0    |1         |0               |1          |48.345169683|2.237202381|\n",
      "|140.0  |6.0      |5.0        |1.0               |315000.0 |0    |0    |0    |0    |0    |0    |1         |0               |1          |48.303340526|2.827154447|\n",
      "|67.0   |3.0      |2.0        |1.0               |190000.0 |0    |0    |0    |0    |0    |0    |1         |1               |0          |49.027507823|1.975650874|\n",
      "+-------+---------+-----------+------------------+---------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 12 — Colonnes finales (dataset ML) + cast + dropna\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "final_cols = [\n",
    "    \"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"Prix\",\n",
    "    \"DPE_B\", \"DPE_C\", \"DPE_D\", \"DPE_E\", \"DPE_F\", \"DPE_G\", \"DPE_Vierge\",\n",
    "    \"Type_Appartement\", \"Type_Maison\",\n",
    "    \"latitude\", \"longitude\"\n",
    "]\n",
    "\n",
    "# Sécurité : si une colonne manque, on la crée à 0\n",
    "for c in final_cols:\n",
    "    if c not in annonces_geo.columns:\n",
    "        annonces_geo = annonces_geo.withColumn(c, F.lit(0))\n",
    "\n",
    "dataset_final = annonces_geo.select(*final_cols)\n",
    "\n",
    "double_cols = [\"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"Prix\", \"latitude\", \"longitude\"]\n",
    "int_cols = [c for c in final_cols if c not in double_cols]\n",
    "\n",
    "for c in double_cols:\n",
    "    dataset_final = dataset_final.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "for c in int_cols:\n",
    "    dataset_final = dataset_final.withColumn(c, F.col(c).cast(IntegerType()))\n",
    "\n",
    "dataset_final = dataset_final.dropna()\n",
    "\n",
    "print(\"Nb lignes (final):\", dataset_final.count())\n",
    "print(\"Nb colonnes (final):\", len(dataset_final.columns))\n",
    "dataset_final.printSchema()\n",
    "dataset_final.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaa1a1-2d29-4d01-8d39-d89f0a5651fe",
   "metadata": {},
   "source": [
    "# Filtre “réaliste” sur le prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb1ae280-d149-4fc1-8693-38cbde57c1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes avant filtre : 503\n",
      "Lignes après filtre : 495\n",
      "+--------+---------+\n",
      "|min_prix|max_prix |\n",
      "+--------+---------+\n",
      "|14000.0 |4220000.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 13 — Filtre “réaliste” sur le prix\n",
    "dataset_final_ml = dataset_final.filter(\n",
    "    (F.col(\"Prix\") >= 10_000) & (F.col(\"Prix\") <= 5_000_000)\n",
    ")\n",
    "\n",
    "print(\"Lignes avant filtre :\", dataset_final.count())\n",
    "print(\"Lignes après filtre :\", dataset_final_ml.count())\n",
    "\n",
    "dataset_final_ml.select(\n",
    "    F.min(\"Prix\").alias(\"min_prix\"),\n",
    "    F.max(\"Prix\").alias(\"max_prix\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ba3ff-6a8d-4433-814e-db7dae9c359e",
   "metadata": {},
   "source": [
    "# Sauvegarde du dataset final (Parquet) + contrôle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26e97a8-cb3e-4387-b702-059c18dbcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sauvegardé en Parquet dans: dataset_final_immo_idf_parquet\n",
      "Relecture OK. Nb lignes: 495 Nb colonnes: 16\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|Surface|NbrPieces|NbrChambres|NbrSdb|Prix    |DPE_B|DPE_C|DPE_D|DPE_E|DPE_F|DPE_G|DPE_Vierge|Type_Appartement|Type_Maison|latitude    |longitude  |\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|60.0   |3.0      |2.0        |1.0   |215000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.60739655 |2.623860846|\n",
      "|70.0   |4.0      |2.0        |1.0   |180000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|75.0   |2.0      |3.0        |1.0   |115000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|70.0   |4.0      |2.0        |1.0   |120000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|90.0   |4.0      |3.0        |1.0   |700000.0|0    |0    |0    |1    |0    |0    |0         |1               |0          |48.836400822|2.239066161|\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule 14 — Sauvegarde du dataset final + contrôle\n",
    "# Parquet (recommandé Spark)\n",
    "(dataset_final_ml\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .parquet(OUTPUT_DATASET_PATH))\n",
    "\n",
    "print(\"Dataset sauvegardé en Parquet dans:\", OUTPUT_DATASET_PATH)\n",
    "\n",
    "# Contrôle : relecture\n",
    "check = spark.read.parquet(OUTPUT_DATASET_PATH)\n",
    "print(\"Relecture OK. Nb lignes:\", check.count(), \"Nb colonnes:\", len(check.columns))\n",
    "check.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
