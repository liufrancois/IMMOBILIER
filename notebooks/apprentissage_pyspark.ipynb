{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cbe843-2525-405e-9cd0-fbf5091c42e4",
   "metadata": {},
   "source": [
    "# Pré-contrôle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb580e-6af5-4063-b389-bb7161e7e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\n",
      "Dataset dir: C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\dataset_final_immo_idf_parquet\n",
      "Hadoop home: C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\n",
      "Bin dir    : C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATASET_DIR_NAME = \"dataset_final_immo_idf_parquet\"\n",
    "HADOOP_DIR_NAME  = \"hadoop\"\n",
    "\n",
    "dataset_dir = (PROJECT_ROOT / \"data\" / \"processed\" / DATASET_DIR_NAME).resolve()\n",
    "hadoop_home = (PROJECT_ROOT / HADOOP_DIR_NAME).resolve()\n",
    "bin_dir     = (hadoop_home / \"bin\").resolve()\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Dataset dir:\", dataset_dir)\n",
    "print(\"Hadoop home:\", hadoop_home)\n",
    "print(\"Bin dir    :\", bin_dir)\n",
    "\n",
    "assert dataset_dir.exists(), f\"Dataset Parquet introuvable: {dataset_dir}\"\n",
    "assert (bin_dir / \"winutils.exe\").exists(), f\"winutils.exe introuvable: {bin_dir / 'winutils.exe'}\"\n",
    "assert (bin_dir / \"hadoop.dll\").exists(),   f\"hadoop.dll introuvable: {bin_dir / 'hadoop.dll'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d44643-20ea-4ab7-8b9c-f7543310789e",
   "metadata": {},
   "source": [
    "# Windows Hadoop (process-only) + priorité DLL + détection “DLL parasite”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f473c-f31c-405a-ac90-4c5b13dab8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME = C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\n",
      "PATH head   = ['C:\\\\Users\\\\ilyes\\\\M2\\\\Algorithmique\\\\Projet\\\\hadoop\\\\bin', 'A:\\\\Apps\\\\ide\\\\Anaconda', 'A:\\\\Apps\\\\ide\\\\Anaconda\\\\Library\\\\mingw-w64\\\\bin']\n",
      "where hadoop.dll ->\n",
      "C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\\hadoop\\bin\\hadoop.dll\n",
      "OK - pas de conflit DLL (ou votre DLL est prioritaire).\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "#Ajouter explicitement le dossier BIN à la recherche de DLL (Python 3.8+)\n",
    "if hasattr(os, \"add_dll_directory\"):\n",
    "    os.add_dll_directory(str(bin_dir))\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = str(hadoop_home)\n",
    "os.environ[\"hadoop.home.dir\"] = str(hadoop_home)\n",
    "\n",
    "os.environ[\"PATH\"] = str(bin_dir) + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "print(\"HADOOP_HOME =\", os.environ[\"HADOOP_HOME\"])\n",
    "print(\"PATH head   =\", os.environ[\"PATH\"].split(os.pathsep)[:3])\n",
    "\n",
    "out = subprocess.run([\"where\", \"hadoop.dll\"], capture_output=True, text=True)\n",
    "dlls = [l.strip() for l in (out.stdout or \"\").splitlines() if l.strip()]\n",
    "print(\"where hadoop.dll ->\")\n",
    "print(\"\\n\".join(dlls) if dlls else (out.stderr or \"Aucune sortie\"))\n",
    "\n",
    "expected = str((bin_dir / \"hadoop.dll\").resolve())\n",
    "if dlls:\n",
    "    first = str(Path(dlls[0]).resolve())\n",
    "    if first != expected:\n",
    "        raise RuntimeError(\n",
    "            \"Conflit DLL: la 1ère hadoop.dll trouvée n'est PAS celle de votre projet.\\n\"\n",
    "            f\"Attendu en 1er: {expected}\\n\"\n",
    "            f\"Trouvé en 1er : {first}\\n\"\n",
    "            \"Solution: supprimez/renommez la hadoop.dll parasite (ou retirez son dossier du PATH), \"\n",
    "            \"puis Kernel Restart et relancez depuis la cellule 0.\"\n",
    "        )\n",
    "print(\"OK - pas de conflit DLL (ou votre DLL est prioritaire).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746d3ba-2f02-4b14-b759-8b1ac5379b0f",
   "metadata": {},
   "source": [
    "# Démarrage Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc5c6a-0885-4f59-9d27-f0cc09a6a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.6\n",
      "Hadoop (JVM): 3.3.4\n",
      "Native loaded?: True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"ImmoIDF-Apprentissage\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"Hadoop (JVM):\", spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "print(\"Native loaded?:\", spark.sparkContext._jvm.org.apache.hadoop.util.NativeCodeLoader.isNativeCodeLoaded())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d589e-518f-4378-aa6a-608a1b7cce24",
   "metadata": {},
   "source": [
    "# FS Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d322ad-b403-4707-82b0-a8b4ecf69bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - listStatus. Nb entrées: 2\n"
     ]
    }
   ],
   "source": [
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "p  = spark._jvm.org.apache.hadoop.fs.Path(str(dataset_dir))  # chemin Windows normal\n",
    "st = fs.listStatus(p)\n",
    "print(\"OK - listStatus. Nb entrées:\", len(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafbbf0-dea4-4534-bd47-14ec03a36d0f",
   "metadata": {},
   "source": [
    "# Lecture du dataset Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b696783-522e-4830-a964-cbc7b76bba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes: 495\n",
      "Nb colonnes: 16\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|Surface|NbrPieces|NbrChambres|NbrSdb|Prix    |DPE_B|DPE_C|DPE_D|DPE_E|DPE_F|DPE_G|DPE_Vierge|Type_Appartement|Type_Maison|latitude    |longitude  |\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "|60.0   |3.0      |2.0        |1.0   |215000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.60739655 |2.623860846|\n",
      "|70.0   |4.0      |2.0        |1.0   |180000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|75.0   |2.0      |3.0        |1.0   |115000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|70.0   |4.0      |2.0        |1.0   |120000.0|0    |0    |0    |0    |0    |0    |1         |1               |0          |48.657051798|2.38747617 |\n",
      "|90.0   |4.0      |3.0        |1.0   |700000.0|0    |0    |0    |1    |0    |0    |0         |1               |0          |48.836400822|2.239066161|\n",
      "+-------+---------+-----------+------+--------+-----+-----+-----+-----+-----+-----+----------+----------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.parquet(str(dataset_dir))\n",
    "df = df.withColumn(\"Prix\", F.col(\"Prix\").cast(\"double\"))\n",
    "\n",
    "print(\"Nb lignes:\", df.count())\n",
    "print(\"Nb colonnes:\", len(df.columns))\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42905477-759e-44fa-b84e-e046e00a33de",
   "metadata": {},
   "source": [
    "# Features/Label (drop baselines + drop constantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec824c-4183-4c70-a32f-70ee2042e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines supprimées: {'Type_Appartement', 'DPE_Vierge'}\n",
      "Constantes supprimées: []\n",
      "Nb features: 13\n",
      "Features: ['Surface', 'NbrPieces', 'NbrChambres', 'NbrSdb', 'DPE_B', 'DPE_C', 'DPE_D', 'DPE_E', 'DPE_F', 'DPE_G', 'Type_Maison', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "label_col = \"Prix\"\n",
    "\n",
    "drop_baselines = {\"Type_Appartement\", \"DPE_Vierge\"}\n",
    "raw_feature_cols = [c for c in df.columns if c != label_col and c not in drop_baselines]\n",
    "\n",
    "const_cols = []\n",
    "for c in raw_feature_cols:\n",
    "    mm = df.agg(F.min(c).alias(\"min\"), F.max(c).alias(\"max\")).first()\n",
    "    if mm[\"min\"] == mm[\"max\"]:\n",
    "        const_cols.append(c)\n",
    "\n",
    "feature_cols = [c for c in raw_feature_cols if c not in const_cols]\n",
    "\n",
    "print(\"Baselines supprimées:\", drop_baselines)\n",
    "print(\"Constantes supprimées:\", const_cols)\n",
    "print(\"Nb features:\", len(feature_cols))\n",
    "print(\"Features:\", feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91028a5-05ea-4404-a539-8c2659da67c2",
   "metadata": {},
   "source": [
    "# VectorAssembler + split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859d75f-617e-46b3-b495-25a259eeddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 370 Test: 125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "data = (assembler\n",
    "        .transform(df)\n",
    "        .select(\"features\", F.col(label_col).alias(\"label\")))\n",
    "\n",
    "train_df, test_df = data.randomSplit([0.75, 0.25], seed=49)\n",
    "\n",
    "train_df = train_df.cache()\n",
    "test_df  = test_df.cache()\n",
    "_ = train_df.count()\n",
    "_ = test_df.count()\n",
    "\n",
    "print(\"Train:\", train_df.count(), \"Test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eaf9b1-9287-46ad-a885-72df1483e4f5",
   "metadata": {},
   "source": [
    "# Évaluateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402a9e2-2f0d-4412-b179-24ad7d12d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "e_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "e_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d89aa9-2de2-42bf-a40c-920f60b8ddb1",
   "metadata": {},
   "source": [
    "# Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07cf9d70-52f1-488c-a131-8b0786dc7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -> R2: -0.03231324664734747 RMSE: 203569.18947120733\n",
      "+---------+------------------+\n",
      "|label    |prediction        |\n",
      "+---------+------------------+\n",
      "|140000.0 |379295.2311125845 |\n",
      "|102000.0 |288502.311502818  |\n",
      "|233000.0 |344821.1686208248 |\n",
      "|244500.0 |401560.2589679044 |\n",
      "|689000.0 |360841.200340122  |\n",
      "|230000.0 |349521.0862640254 |\n",
      "|635000.0 |379097.26784938015|\n",
      "|560000.0 |381912.1592435427 |\n",
      "|475000.0 |376110.46623755805|\n",
      "|1600000.0|362770.11282882094|\n",
      "+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    solver=\"l-bfgs\",\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0,\n",
    "    maxIter=200\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "pred_lr = lr_model.transform(test_df)\n",
    "\n",
    "r2_lr   = float(e_r2.evaluate(pred_lr))\n",
    "rmse_lr = float(e_rmse.evaluate(pred_lr))\n",
    "\n",
    "print(\"LR -> R2:\", r2_lr, \"RMSE:\", rmse_lr)\n",
    "pred_lr.select(\"label\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a081b2-6d83-43bf-9463-76041cca31ea",
   "metadata": {},
   "source": [
    "# Scaling + LR (MinMax et Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c8637-5663-4eff-995a-8812d4ccbfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax+LR -> R2: -0.03231324660475332 RMSE: 203569.1894670076\n",
      "Std+LR -> R2: -0.03231324657738632 RMSE: 203569.18946430925\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#MinMax + LR\n",
    "minmax = MinMaxScaler(inputCol=\"features\", outputCol=\"features_minmax\")\n",
    "lr_mm = LinearRegression(featuresCol=\"features_minmax\", labelCol=\"label\", solver=\"l-bfgs\", regParam=0.0, maxIter=200)\n",
    "pipe_mm = Pipeline(stages=[minmax, lr_mm])\n",
    "m_mm = pipe_mm.fit(train_df)\n",
    "pred_mm = m_mm.transform(test_df)\n",
    "r2_mm   = float(e_r2.evaluate(pred_mm))\n",
    "rmse_mm = float(e_rmse.evaluate(pred_mm))\n",
    "print(\"MinMax+LR -> R2:\", r2_mm, \"RMSE:\", rmse_mm)\n",
    "\n",
    "#Std + LR\n",
    "std = StandardScaler(inputCol=\"features\", outputCol=\"features_std\", withStd=True, withMean=False)\n",
    "lr_std = LinearRegression(featuresCol=\"features_std\", labelCol=\"label\", solver=\"l-bfgs\", regParam=0.0, maxIter=200)\n",
    "pipe_std = Pipeline(stages=[std, lr_std])\n",
    "m_std = pipe_std.fit(train_df)\n",
    "pred_std = m_std.transform(test_df)\n",
    "r2_std   = float(e_r2.evaluate(pred_std))\n",
    "rmse_std = float(e_rmse.evaluate(pred_std))\n",
    "print(\"Std+LR -> R2:\", r2_std, \"RMSE:\", rmse_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93677e09-7e76-4bd8-9616-b92dcd6474a3",
   "metadata": {},
   "source": [
    "# K plus proches voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c38861-986f-4bc4-a8cd-d458cb67daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN k=4 -> none   : R2=-0.3507 RMSE=232855.87\n",
      "KNN k=4 -> minmax : R2=-0.6041 RMSE=253762.54\n",
      "KNN k=4 -> std    : R2=-0.2331 RMSE=222488.75\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "\n",
    "#Distance euclidienne\n",
    "def euclidean_py(a, b):\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    return float(math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b))))\n",
    "\n",
    "euclidean_udf = F.udf(euclidean_py, DoubleType())\n",
    "\n",
    "def scale_train_test(train_df, test_df, method: str):\n",
    "    \"\"\"\n",
    "    method in {\"none\", \"minmax\", \"std\"}\n",
    "    Retourne (train_scaled, test_scaled, feat_col)\n",
    "    \"\"\"\n",
    "    if method == \"none\":\n",
    "        return train_df, test_df, \"features\"\n",
    "\n",
    "    if method == \"minmax\":\n",
    "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "        m = scaler.fit(train_df)\n",
    "        return m.transform(train_df), m.transform(test_df), \"features_scaled\"\n",
    "\n",
    "    if method == \"std\":\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=False)\n",
    "        m = scaler.fit(train_df)\n",
    "        return m.transform(train_df), m.transform(test_df), \"features_scaled\"\n",
    "\n",
    "    raise ValueError(\"method doit être: none|minmax|std\")\n",
    "\n",
    "def knn_predict_exact(train_df, test_df, k: int, feat_col: str):\n",
    "    \"\"\"\n",
    "    KNN régression : moyenne des k voisins les plus proches (distance euclidienne).\n",
    "    Corrigé : labels renommés pour éviter AMBIGUOUS_REFERENCE.\n",
    "    \"\"\"\n",
    "    tr = (train_df\n",
    "          .withColumn(\"id_train\", F.monotonically_increasing_id())\n",
    "          .withColumn(\"fa\", vector_to_array(F.col(feat_col)))\n",
    "          .select(\"id_train\", \"fa\", F.col(\"label\").alias(\"label_train\")))\n",
    "\n",
    "    te = (test_df\n",
    "          .withColumn(\"id_test\", F.monotonically_increasing_id())\n",
    "          .withColumn(\"fb\", vector_to_array(F.col(feat_col)))\n",
    "          .select(\"id_test\", \"fb\", F.col(\"label\").alias(\"label_test\")))\n",
    "\n",
    "    pairs = (te.crossJoin(tr)\n",
    "             .withColumn(\"dist\", euclidean_udf(F.col(\"fb\"), F.col(\"fa\")))\n",
    "             .select(\"id_test\", \"label_test\", \"label_train\", \"dist\"))\n",
    "\n",
    "    w = Window.partitionBy(\"id_test\").orderBy(F.col(\"dist\").asc())\n",
    "    knn = pairs.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\") <= k)\n",
    "\n",
    "    pred = (knn.groupBy(\"id_test\")\n",
    "            .agg(F.avg(\"label_train\").alias(\"prediction\")))\n",
    "\n",
    "    out = (te.select(\"id_test\", \"label_test\")\n",
    "           .join(pred, on=\"id_test\", how=\"inner\")\n",
    "           .select(F.col(\"label_test\").alias(\"label\"), \"prediction\"))\n",
    "\n",
    "    return out\n",
    "\n",
    "def eval_knn(train_df, test_df, k: int, scaling: str):\n",
    "    tr_s, te_s, feat_col = scale_train_test(train_df, test_df, scaling)\n",
    "    pred = knn_predict_exact(tr_s, te_s, k=k, feat_col=feat_col)\n",
    "    r2 = float(e_r2.evaluate(pred))\n",
    "    rmse = float(e_rmse.evaluate(pred))\n",
    "    return r2, rmse\n",
    "\n",
    "k = 4\n",
    "r2_knn_none, rmse_knn_none = eval_knn(train_df, test_df, k=k, scaling=\"none\")\n",
    "r2_knn_mm,   rmse_knn_mm   = eval_knn(train_df, test_df, k=k, scaling=\"minmax\")\n",
    "r2_knn_std,  rmse_knn_std  = eval_knn(train_df, test_df, k=k, scaling=\"std\")\n",
    "\n",
    "print(f\"KNN k={k} -> none   : R2={r2_knn_none:.4f} RMSE={rmse_knn_none:.2f}\")\n",
    "print(f\"KNN k={k} -> minmax : R2={r2_knn_mm:.4f} RMSE={rmse_knn_mm:.2f}\")\n",
    "print(f\"KNN k={k} -> std    : R2={r2_knn_std:.4f} RMSE={rmse_knn_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7a331-c097-4e23-91c1-e904137978cc",
   "metadata": {},
   "source": [
    "# Comparatif final (LR vs KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf410fee-e09c-475a-a4bf-d0990327eccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|Modele    |R2                  |RMSE              |\n",
      "+----------+--------------------+------------------+\n",
      "|Std+LR    |-0.03231324657738632|203569.18946430925|\n",
      "|MinMax+LR |-0.03231324660475332|203569.1894670076 |\n",
      "|LR        |-0.03231324664734747|203569.18947120733|\n",
      "|Std+KNN   |-0.23311480247608518|222488.74838266318|\n",
      "|KNN       |-0.3507089791675315 |232855.87021257592|\n",
      "|MinMax+KNN|-0.6041405396270276 |253762.53524702974|\n",
      "+----------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [\n",
    "    (\"LR\", r2_lr, rmse_lr),\n",
    "]\n",
    "\n",
    "if \"r2_mm\" in globals():\n",
    "    rows.append((\"MinMax+LR\", r2_mm, rmse_mm))\n",
    "if \"r2_std\" in globals():\n",
    "    rows.append((\"Std+LR\", r2_std, rmse_std))\n",
    "\n",
    "#KNN\n",
    "rows += [\n",
    "    (\"KNN\", r2_knn_none, rmse_knn_none),\n",
    "    (\"MinMax+KNN\", r2_knn_mm, rmse_knn_mm),\n",
    "    (\"Std+KNN\", r2_knn_std, rmse_knn_std),\n",
    "]\n",
    "\n",
    "results = spark.createDataFrame(rows, [\"Modele\", \"R2\", \"RMSE\"]).orderBy(F.col(\"RMSE\").asc())\n",
    "results.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d14c9-a3bc-4040-b573-86c90ae45fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
