{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f57326f-c1e2-47b1-8f2a-e57e294a2d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\ilyes\\M2\\Algorithmique\\Projet\n",
      "Fichiers: ['.ipynb_checkpoints', 'cities.csv', 'idf_ventes.csv', 'nettoyage_pyspark.ipynb', 'scrap.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Fichiers:\", os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdefef3-99d7-4703-8ea5-ad370700c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"ImmoIDF-Nettoyage\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c48e6-376d-4096-b40d-c4921a3b4478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Colonnes ===\n",
      "['Ville', 'Type', 'Surface', 'NbrPieces', 'NbrChambres', 'NbrSdb', 'DPE', 'Prix']\n",
      "Missing: []\n",
      "Extra: []\n",
      "\n",
      "=== Schéma ===\n",
      "root\n",
      " |-- Ville: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Surface: string (nullable = true)\n",
      " |-- NbrPieces: string (nullable = true)\n",
      " |-- NbrChambres: string (nullable = true)\n",
      " |-- NbrSdb: string (nullable = true)\n",
      " |-- DPE: string (nullable = true)\n",
      " |-- Prix: string (nullable = true)\n",
      "\n",
      "\n",
      "=== Aperçu ===\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "|Ville             |Type  |Surface|NbrPieces|NbrChambres|NbrSdb|DPE   |Prix  |\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "|Paris 15ème       |Maison|220    |8        |4          |3     |Vierge|250000|\n",
      "|Meaux             |Maison|325    |10       |5          |-     |-     |749000|\n",
      "|Misy-sur-Yonne    |Maison|128    |5        |4          |1     |E     |279000|\n",
      "|Nanteuil-lès-Meaux|Maison|89     |5        |3          |-     |F     |242000|\n",
      "|Villeparisis      |Maison|65     |3        |2          |1     |B     |289000|\n",
      "|Bondy             |Maison|118    |5        |3          |1     |B     |500000|\n",
      "|Montreuil         |Maison|160    |7        |4          |2     |E     |695000|\n",
      "|Argenteuil        |Maison|43     |2        |1          |-     |C     |199000|\n",
      "|Eaubonne          |Maison|94     |5        |3          |1     |G     |410000|\n",
      "|Franconville      |Maison|190    |7        |4          |2     |C     |599990|\n",
      "+------------------+------+-------+---------+-----------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "=== Nb lignes ===\n",
      "530\n",
      "\n",
      "=== Null counts (devrait être 0 partout si CSV bien formé) ===\n",
      "+----------+---------+------------+--------------+----------------+-----------+--------+---------+\n",
      "|null_Ville|null_Type|null_Surface|null_NbrPieces|null_NbrChambres|null_NbrSdb|null_DPE|null_Prix|\n",
      "+----------+---------+------------+--------------+----------------+-----------+--------+---------+\n",
      "|0         |0        |0           |0             |0               |0          |0       |0        |\n",
      "+----------+---------+------------+--------------+----------------+-----------+--------+---------+\n",
      "\n",
      "\n",
      "=== Distinct Type / DPE ===\n",
      "+-----------+\n",
      "|Type       |\n",
      "+-----------+\n",
      "|Appartement|\n",
      "|Maison     |\n",
      "+-----------+\n",
      "\n",
      "+------+\n",
      "|DPE   |\n",
      "+------+\n",
      "|F     |\n",
      "|E     |\n",
      "|B     |\n",
      "|D     |\n",
      "|C     |\n",
      "|-     |\n",
      "|Vierge|\n",
      "|G     |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, count, when\n",
    "\n",
    "EXPECTED_COLS = [\"Ville\", \"Type\", \"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"DPE\", \"Prix\"]\n",
    "\n",
    "annonces = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"utf-8\")\n",
    "    .csv(\"data/raw/idf_ventes.csv\")\n",
    ")\n",
    "\n",
    "# Trim sur toutes les colonnes (conformité)\n",
    "for c in annonces.columns:\n",
    "    annonces = annonces.withColumn(c, trim(col(c)))\n",
    "\n",
    "print(\"=== Colonnes ===\")\n",
    "print(annonces.columns)\n",
    "\n",
    "missing = [c for c in EXPECTED_COLS if c not in annonces.columns]\n",
    "extra = [c for c in annonces.columns if c not in EXPECTED_COLS]\n",
    "print(\"Missing:\", missing)\n",
    "print(\"Extra:\", extra)\n",
    "\n",
    "print(\"\\n=== Schéma ===\")\n",
    "annonces.printSchema()\n",
    "\n",
    "print(\"\\n=== Aperçu ===\")\n",
    "annonces.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n=== Nb lignes ===\")\n",
    "print(annonces.count())\n",
    "\n",
    "print(\"\\n=== Null counts (devrait être 0 partout si CSV bien formé) ===\")\n",
    "annonces.select([count(when(col(c).isNull(), 1)).alias(f\"null_{c}\") for c in EXPECTED_COLS]).show(truncate=False)\n",
    "\n",
    "print(\"\\n=== Distinct Type / DPE ===\")\n",
    "annonces.select(\"Type\").distinct().show(truncate=False)\n",
    "annonces.select(\"DPE\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad5ab0a-aef3-4e62-8f7d-20749f4dfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annonces = annonces.replace(\"-\", \"Vierge\", subset=[\"DPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6c3387-5fe0-4248-80ef-a1114a84d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|DPE   |\n",
      "+------+\n",
      "|F     |\n",
      "|E     |\n",
      "|B     |\n",
      "|D     |\n",
      "|C     |\n",
      "|Vierge|\n",
      "|G     |\n",
      "+------+\n",
      "\n",
      "Nb DPE='-': 0\n"
     ]
    }
   ],
   "source": [
    "annonces.select(\"DPE\").distinct().show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Nb DPE='-':\", annonces.filter(col(\"DPE\") == \"-\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66849f07-f6ec-4e46-b958-7057ebfe13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "num_cols = [\"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\"]\n",
    "\n",
    "for c in num_cols:\n",
    "    annonces = annonces.withColumn(\n",
    "        c,\n",
    "        when((col(c) == \"-\") | (col(c) == \"\") | col(c).isNull(), None).otherwise(col(c))\n",
    "    ).withColumn(c, col(c).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5415b486-e1de-4832-8a4c-72f4e37bf549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyennes: {'Surface': 91.27362204724409, 'NbrPieces': 5.5265225933202355, 'NbrChambres': 2.525879917184265, 'NbrSdb': 1.2905405405405406}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, lit\n",
    "\n",
    "means_row = annonces.agg(*[avg(col(c)).alias(c) for c in num_cols]).collect()[0]\n",
    "means = {c: means_row[c] for c in num_cols}\n",
    "print(\"Moyennes:\", means)\n",
    "\n",
    "for c in num_cols:\n",
    "    annonces = annonces.withColumn(c, when(col(c).isNull(), lit(means[c])).otherwise(col(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0538bfea-9a39-46a7-a764-35de1f498c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------------+-----------+\n",
      "|null_Surface|null_NbrPieces|null_NbrChambres|null_NbrSdb|\n",
      "+------------+--------------+----------------+-----------+\n",
      "|0           |0             |0               |0          |\n",
      "+------------+--------------+----------------+-----------+\n",
      "\n",
      "Nb lignes après dropna: 530\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Compter les nulls restants\n",
    "null_counts = annonces.select([count(when(col(c).isNull(), 1)).alias(f\"null_{c}\") for c in num_cols])\n",
    "null_counts.show(truncate=False)\n",
    "\n",
    "# S'il reste des nulls (ex: colonne entièrement vide => moyenne = None), on supprime ces lignes\n",
    "annonces = annonces.dropna(subset=num_cols)\n",
    "\n",
    "print(\"Nb lignes après dropna:\", annonces.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00edbc51-2bf8-43c0-b68f-181b4fc3f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPE distinct: ['B', 'C', 'D', 'E', 'F', 'G', 'Vierge']\n",
      "Type distinct: ['Appartement', 'Maison']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dpe_vals  = [r[0] for r in annonces.select(\"DPE\").distinct().collect()]\n",
    "type_vals = [r[0] for r in annonces.select(\"Type\").distinct().collect()]\n",
    "\n",
    "print(\"DPE distinct:\", sorted(dpe_vals))\n",
    "print(\"Type distinct:\", sorted(type_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b27cc47-d39c-4bcc-9d39-b2c5a67859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "import re\n",
    "\n",
    "def _safe_colname(prefix: str, value: str) -> str:\n",
    "    # Nettoyage minimal pour faire un nom de colonne Spark valide\n",
    "    v = str(value).strip()\n",
    "    v = v.replace(\" \", \"_\")\n",
    "    v = re.sub(r\"[^0-9A-Za-z_]\", \"_\", v)  # remplace accents/symboles par _\n",
    "    v = re.sub(r\"_+\", \"_\", v).strip(\"_\")\n",
    "    return f\"{prefix}_{v}\"\n",
    "\n",
    "# DPE -> colonnes DPE_*\n",
    "for v in dpe_vals:\n",
    "    cname = _safe_colname(\"DPE\", v)\n",
    "    annonces = annonces.withColumn(cname, when(col(\"DPE\") == lit(v), 1).otherwise(0).cast(\"int\"))\n",
    "\n",
    "# Type -> colonnes Type_*\n",
    "for v in type_vals:\n",
    "    cname = _safe_colname(\"Type\", v)\n",
    "    annonces = annonces.withColumn(cname, when(col(\"Type\") == lit(v), 1).otherwise(0).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9b05b4-c3e3-47b6-8eba-b2b112cebdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-----+-----+-----+-----+----------+-----+----------------+-----------+\n",
      "|Type  |DPE   |DPE_F|DPE_E|DPE_B|DPE_D|DPE_C|DPE_Vierge|DPE_G|Type_Appartement|Type_Maison|\n",
      "+------+------+-----+-----+-----+-----+-----+----------+-----+----------------+-----------+\n",
      "|Maison|Vierge|0    |0    |0    |0    |0    |1         |0    |0               |1          |\n",
      "|Maison|Vierge|0    |0    |0    |0    |0    |1         |0    |0               |1          |\n",
      "|Maison|E     |0    |1    |0    |0    |0    |0         |0    |0               |1          |\n",
      "|Maison|F     |1    |0    |0    |0    |0    |0         |0    |0               |1          |\n",
      "|Maison|B     |0    |0    |1    |0    |0    |0         |0    |0               |1          |\n",
      "|Maison|B     |0    |0    |1    |0    |0    |0         |0    |0               |1          |\n",
      "|Maison|E     |0    |1    |0    |0    |0    |0         |0    |0               |1          |\n",
      "|Maison|C     |0    |0    |0    |0    |1    |0         |0    |0               |1          |\n",
      "|Maison|G     |0    |0    |0    |0    |0    |0         |1    |0               |1          |\n",
      "|Maison|C     |0    |0    |0    |0    |1    |0         |0    |0               |1          |\n",
      "+------+------+-----+-----+-----+-----+-----+----------+-----+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affiche quelques colonnes indicatrices\n",
    "cols_to_show = [\"Type\", \"DPE\"] + [c for c in annonces.columns if c.startswith(\"Type_\") or c.startswith(\"DPE_\")]\n",
    "annonces.select(cols_to_show).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9015f2c-8c1b-4f93-af8e-13ef33929c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+------+--------+-------------+\n",
      "|DPE   |DPE_idx|DPE_ohe      |Type  |Type_idx|Type_ohe     |\n",
      "+------+-------+-------------+------+--------+-------------+\n",
      "|Vierge|0.0    |(7,[0],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|Vierge|0.0    |(7,[0],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|E     |2.0    |(7,[2],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|F     |4.0    |(7,[4],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|B     |5.0    |(7,[5],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|B     |5.0    |(7,[5],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|E     |2.0    |(7,[2],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|C     |3.0    |(7,[3],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|G     |6.0    |(7,[6],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "|C     |3.0    |(7,[3],[1.0])|Maison|1.0     |(2,[1],[1.0])|\n",
      "+------+-------+-------------+------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"DPE\", outputCol=\"DPE_idx\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Type\", outputCol=\"Type_idx\", handleInvalid=\"keep\"),\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=\"DPE_idx\", outputCol=\"DPE_ohe\"),\n",
    "    OneHotEncoder(inputCol=\"Type_idx\", outputCol=\"Type_ohe\"),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(stages=indexers + encoders)\n",
    "model = pipe.fit(annonces)\n",
    "annonces_ohe = model.transform(annonces)\n",
    "\n",
    "annonces_ohe.select(\"DPE\", \"DPE_idx\", \"DPE_ohe\", \"Type\", \"Type_idx\", \"Type_ohe\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d0d37-4733-4a69-aa8b-5a49fba56c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'cities.csv', 'idf_ventes.csv', 'nettoyage_pyspark.ipynb', 'scrap.py']\n",
      "Colonnes villes: ['insee_code', 'city_code', 'zip_code', 'label', 'latitude', 'longitude', 'department_name', 'department_number', 'region_name', 'region_geojson_name']\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "|insee_code|city_code          |zip_code|label              |latitude    |longitude  |department_name|department_number|region_name            |region_geojson_name    |\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "|25620     |ville du pont      |25650   |ville du pont      |46.999873398|6.498147193|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25624     |villers grelot     |25640   |villers grelot     |47.361512085|6.235167025|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25615     |villars les blamont|25310   |villars les blamont|47.368383721|6.871414913|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25619     |les villedieu      |25240   |les villedieu      |46.713906258|6.26583065 |doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "|25622     |villers buzon      |25170   |villers buzon      |47.228558434|5.852186748|doubs          |25               |bourgogne-franche-comté|Bourgogne-Franche-Comté|\n",
      "+----------+-------------------+--------+-------------------+------------+-----------+---------------+-----------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- insee_code: string (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- department_number: string (nullable = true)\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- region_geojson_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(os.listdir(\".\"))  # vérifiez que cities.csv est bien là\n",
    "\n",
    "# Essai séparateur ';'\n",
    "villes = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"encoding\", \"utf-8\")\n",
    "          .option(\"sep\", \";\")\n",
    "          .csv(\"data/processed/cities.csv\"))\n",
    "\n",
    "# Si une seule colonne, on retente avec ','\n",
    "if len(villes.columns) == 1:\n",
    "    villes = (spark.read\n",
    "              .option(\"header\", True)\n",
    "              .option(\"encoding\", \"utf-8\")\n",
    "              .option(\"sep\", \",\")\n",
    "              .csv(\"data/processed/cities.csv\"))\n",
    "\n",
    "print(\"Colonnes villes:\", villes.columns)\n",
    "villes.show(5, truncate=False)\n",
    "villes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "167e8a93-4df6-4ba5-8eb9-c7b4688280c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def strip_accents_py(s: str) -> str:\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "strip_accents = F.udf(strip_accents_py, StringType())\n",
    "\n",
    "def add_city_key(df, colname: str, out: str = \"Ville_key\"):\n",
    "    x = F.lower(F.trim(F.col(colname)))\n",
    "    x = strip_accents(x)\n",
    "    x = F.regexp_replace(x, r\"\\b\\d+\\s*(er|e|eme|ème)\\b\", \"\")   # arrondissements\n",
    "    x = F.regexp_replace(x, r\"[^0-9a-z]+\", \" \")               # séparateurs -> espace\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    x = F.trim(x)\n",
    "    return df.withColumn(out, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688c31b8-1263-4bc7-bbfa-d3629bb1d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+\n",
      "|Ville_key          |latitude    |longitude  |\n",
      "+-------------------+------------+-----------+\n",
      "|barbizon           |48.448347603|2.600809608|\n",
      "|cergy              |49.039967587|2.051139021|\n",
      "|coulommiers        |48.81234041 |3.091269785|\n",
      "|reau               |48.60739655 |2.623860846|\n",
      "|fontenay le vicomte|48.547225879|2.400248804|\n",
      "+-------------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IDF_DEPTS = [\"75\",\"77\",\"78\",\"91\",\"92\",\"93\",\"94\",\"95\"]\n",
    "\n",
    "villes_idf = villes.filter(F.col(\"department_number\").isin(IDF_DEPTS))\n",
    "\n",
    "villes_k = (add_city_key(villes_idf, \"label\", \"Ville_key\")\n",
    "            .select(\n",
    "                \"Ville_key\",\n",
    "                F.col(\"latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "                F.col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "            )\n",
    "            .groupBy(\"Ville_key\")\n",
    "            .agg(F.avg(\"latitude\").alias(\"latitude\"), F.avg(\"longitude\").alias(\"longitude\"))\n",
    "           )\n",
    "\n",
    "villes_k.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c98a65-ad37-46fb-8ecc-eab181692365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes annonces_geo: 530\n",
      "Nb lignes sans coordonnées (avant suppression): 27\n"
     ]
    }
   ],
   "source": [
    "annonces_tmp = add_city_key(annonces_ohe, \"Ville\", \"Ville_key\")\n",
    "\n",
    "annonces_tmp = annonces_tmp.withColumn(\n",
    "    \"Ville_key\",\n",
    "    F.when(F.col(\"Ville_key\").isin(\"evry\", \"courcouronnes\"), F.lit(\"evry courcouronnes\"))\n",
    "     .when(F.col(\"Ville_key\") == \"le chesnay\", F.lit(\"le chesnay rocquencourt\"))\n",
    "     .otherwise(F.col(\"Ville_key\"))\n",
    ")\n",
    "\n",
    "annonces_geo = annonces_tmp.join(villes_k, on=\"Ville_key\", how=\"left\")\n",
    "\n",
    "print(\"Nb lignes annonces_geo:\", annonces_geo.count())\n",
    "print(\"Nb lignes sans coordonnées (avant suppression):\",\n",
    "      annonces_geo.filter(F.col(\"latitude\").isNull() | F.col(\"longitude\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678dc27f-a3c0-4957-bc9f-145651e467de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes après suppression lat/lon manquants: 503\n"
     ]
    }
   ],
   "source": [
    "annonces_geo = annonces_geo.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "print(\"Nb lignes après suppression lat/lon manquants:\", annonces_geo.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e040c44-bbfb-4a53-af68-005d9ce23af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes: 503\n",
      "Nb colonnes: 16\n",
      "root\n",
      " |-- Surface: double (nullable = true)\n",
      " |-- NbrPieces: double (nullable = true)\n",
      " |-- NbrChambres: double (nullable = true)\n",
      " |-- NbrSdb: double (nullable = true)\n",
      " |-- Prix: double (nullable = true)\n",
      " |-- DPE_B: integer (nullable = false)\n",
      " |-- DPE_C: integer (nullable = false)\n",
      " |-- DPE_D: integer (nullable = false)\n",
      " |-- DPE_E: integer (nullable = false)\n",
      " |-- DPE_F: integer (nullable = false)\n",
      " |-- DPE_G: integer (nullable = false)\n",
      " |-- DPE_Vierge: integer (nullable = false)\n",
      " |-- Type_Appartement: integer (nullable = false)\n",
      " |-- Type_Maison: integer (nullable = false)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_cols = [\n",
    "    \"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"Prix\",\n",
    "    \"DPE_B\", \"DPE_C\", \"DPE_D\", \"DPE_E\", \"DPE_F\", \"DPE_G\", \"DPE_Vierge\",\n",
    "    \"Type_Appartement\", \"Type_Maison\",\n",
    "    \"latitude\", \"longitude\"\n",
    "]\n",
    "\n",
    "for c in final_cols:\n",
    "    if c not in annonces_geo.columns:\n",
    "        annonces_geo = annonces_geo.withColumn(c, F.lit(0))\n",
    "\n",
    "annonces = annonces_geo.select(*final_cols)\n",
    "\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "double_cols = [\"Surface\", \"NbrPieces\", \"NbrChambres\", \"NbrSdb\", \"Prix\", \"latitude\", \"longitude\"]\n",
    "int_cols = [c for c in final_cols if c not in double_cols]\n",
    "\n",
    "for c in double_cols:\n",
    "    annonces = annonces.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "for c in int_cols:\n",
    "    annonces = annonces.withColumn(c, F.col(c).cast(IntegerType()))\n",
    "\n",
    "annonces = annonces.dropna()\n",
    "\n",
    "print(\"Nb lignes:\", annonces.count())\n",
    "print(\"Nb colonnes:\", len(annonces.columns))\n",
    "annonces.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d782077-b93a-4fe7-8f13-74418a99cc31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
